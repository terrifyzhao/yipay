{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', 100)\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_base = pd.read_csv('data/train_base.csv')\n",
    "test_a_base = pd.read_csv('data/test_a_base.csv')\n",
    "\n",
    "train_op = pd.read_csv('data/train_op.csv')\n",
    "test_a_op = pd.read_csv('data/test_a_op.csv')\n",
    "\n",
    "train_trans = pd.read_csv('data/train_trans.csv')\n",
    "test_a_trans = pd.read_csv('data/test_a_trans.csv')\n",
    "\n",
    "train_label = pd.read_csv('data/train_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_base = pd.read_csv('data/train_base.csv')\n",
    "train_base = pd.merge(train_base, train_label, on='user', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 填充nan，去除user列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_base_no_nan = train_base.drop(columns=['service3_level'],axis=1)\n",
    "# 离散值填充众数\n",
    "train_base_no_nan['sex'].fillna('category 0',inplace=True)\n",
    "train_base_no_nan['balance_avg'].fillna('level 1',inplace=True)\n",
    "train_base_no_nan['balance1_avg'].fillna('level 1',inplace=True)\n",
    "train_base_no_nan['balance2_avg'].fillna('level 1',inplace=True)\n",
    "\n",
    "user = train_base_no_nan['user'].values\n",
    "train_base_no_user = train_base_no_nan.drop(columns=['user'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "category_columns_name = ['sex','provider','level','verified','agreement1','agreement2','agreement3','agreement4','service3',\n",
    "                        'product3_amount','product4_amount','product5_amount']\n",
    "\n",
    "one_hot_df = pd.get_dummies(train_base_no_user[category_columns_name])\n",
    "\n",
    "one_hot_df['agreement_all_0'] = one_hot_df['agreement1_category 0'].values & one_hot_df['agreement2_category 0'].values & \\\n",
    "                                 one_hot_df['agreement3_category 0'].values & one_hot_df['agreement4_category 0'].values\n",
    "one_hot_df['agreement_all_1'] = one_hot_df['agreement1_category 1'].values & one_hot_df['agreement2_category 1'].values & \\\n",
    "                                 one_hot_df['agreement3_category 1'].values & one_hot_df['agreement4_category 1'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9bafe52aa99e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcity_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'city_mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprovince_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'province_mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mregist_type_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'regist_type_mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbalance_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balance_mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbalance_avg_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balance_avg_mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'city_mean'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'city_mean'",
     "output_type": "error"
    }
   ],
   "source": [
    "city_mean = joblib.load('city_mean')[0]\n",
    "province_mean = joblib.load('province_mean')[0]\n",
    "regist_type_mean = joblib.load('regist_type_mean')[0]\n",
    "balance_mean = joblib.load('balance_mean')[0]\n",
    "balance_avg_mean = joblib.load('balance_avg_mean')[0]\n",
    "balance1_mean = joblib.load('balance1_mean')[0]\n",
    "balance1_avg_mean = joblib.load('balance1_avg_mean')[0]\n",
    "balance2_mean = joblib.load('balance2_mean')[0]\n",
    "balance2_avg_mean = joblib.load('balance2_avg_mean')[0]\n",
    "product1_amount_mean = joblib.load('product1_amount_mean')[0]\n",
    "product2_amount_mean = joblib.load('product2_amount_mean')[0]\n",
    "product6_amount_mean = joblib.load('product6_amount_mean')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_base_categoty = pd.DataFrame()\n",
    "train_base_categoty['city'] = city_mean\n",
    "train_base_categoty['province'] = province_mean\n",
    "train_base_categoty['regist_type'] = regist_type_mean\n",
    "train_base_categoty['balance'] = balance_mean\n",
    "train_base_categoty['balance_avg'] = balance_avg_mean\n",
    "train_base_categoty['balance1'] = balance1_mean\n",
    "train_base_categoty['balance1_avg'] = balance1_avg_mean\n",
    "train_base_categoty['balance2'] = balance2_mean\n",
    "train_base_categoty['balance2_avg'] = balance2_avg_mean\n",
    "train_base_categoty['product1_amount'] = product1_amount_mean\n",
    "train_base_categoty['product2_amount'] = product2_amount_mean\n",
    "train_base_categoty['product6_amount'] = product6_amount_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def magic_feature(df, f1, f2):\n",
    "    df[f'{f1}_{f2}_a'] = df[f1]+df[f2]\n",
    "    df[f'{f1}_{f2}_s'] = df[f1]-df[f2]\n",
    "    df[f'{f1}_{f2}_m'] = df[f1]*df[f2]\n",
    "    df[f'{f1}_{f2}_d'] = df[f1]/df[f2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "magic_feature(train_base_categoty,'city','province')\n",
    "magic_feature(train_base_categoty,'balance1','balance2')\n",
    "magic_feature(train_base_categoty,'balance1_avg','balance2_avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_base_categoty['product_amount_a'] = train_base_categoty['product1_amount']+train_base_categoty['product2_amount']+ \\\n",
    "                                        train_base_categoty['product6_amount']\n",
    "train_base_categoty['product_amount_m'] = train_base_categoty['product1_amount']*train_base_categoty['product2_amount']* \\\n",
    "                                        train_base_categoty['product6_amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连续变量的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_value = train_base_no_user.select_dtypes('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_value['product7_success_cnt'] = df_value['product7_cnt']-df_value['product7_fail_cnt']\n",
    "\n",
    "df_value['card_cnt'] = df_value['card_a_cnt']+df_value['card_b_cnt']+df_value['card_c_cnt']+df_value['card_d_cnt']\n",
    "\n",
    "df_value['ip_cnt_avg'] = df_value['ip_cnt']/df_value['login_days_cnt']\n",
    "\n",
    "df_value['login_cnt_period1_avg']=df_value['login_cnt_period1']/df_value['login_days_cnt']\n",
    "df_value['login_cnt_period2_avg']=df_value['login_cnt_period2']/df_value['login_days_cnt']\n",
    "df_value['login_cnt_period']=df_value['login_cnt_period1']+df_value['login_cnt_period2']\n",
    "df_value['login_cnt_period_avg']=df_value['login_cnt_period']/df_value['login_days_cnt']\n",
    "\n",
    "df_value['service_cnt']=df_value['service1_cnt']+df_value['service2_cnt']\n",
    "df_value['service_avg1_amt']=df_value['service1_amt']/df_value['service1_cnt']\n",
    "\n",
    "df_value['op_cnt']=df_value['op1_cnt']+df_value['op2_cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 归一化\n",
    "df_value = (df_value-df_value.min())/(df_value.max()-df_value.min())\n",
    "\n",
    "df_base = pd.DataFrame()\n",
    "df_base['user'] = user\n",
    "df_base = pd.concat([df_base, one_hot_df, train_base_categoty, df_value],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(df_base.shape)\n",
    "df_base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OP处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "op_df = pd.DataFrame()\n",
    "group = train_op.groupby(['user']).count()\n",
    "op_df['user'] = group.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_op.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "count_column=train_op.columns[1:-1]\n",
    "print(count_column)\n",
    "for column in count_column:\n",
    "    op_df['op_'+column+'_count'] = train_op.groupby('user')[column].count().values\n",
    "    op_df['op_'+column+'_nunique'] = train_op.groupby('user')[column].nunique().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "def timedelta2sec(delta):\n",
    "    day = delta.split('days')[0].strip()\n",
    "    h,m,s = delta.split('days')[1].strip().split(':')\n",
    "    sec=timedelta(days=int(day),hours=int(h),minutes=int(m),seconds=float(s)).total_seconds()\n",
    "    return sec\n",
    "\n",
    "train_op['time_diff_sec'] = train_op['tm_diff'].apply(timedelta2sec)\n",
    "\n",
    "for operate in ['max','min','mean','median','std']:\n",
    "    op_df['op_time_'+operate]=train_op.groupby('user')['time_diff_sec'].agg(operate).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "op_df.fillna(0,inplace=True)\n",
    "print(op_df.shape)\n",
    "op_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交易处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "trans_df = pd.DataFrame()\n",
    "group = train_trans.groupby(['user']).count()\n",
    "trans_df['user'] = group.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 离散列，全部统计有几个类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "count_column = list(train_trans.columns[1:4])+list(train_trans.columns[5:9]) \n",
    "for column in count_column:\n",
    "    print(column)\n",
    "    trans_df['trans_'+column+'_count'] = train_trans.groupby('user')[column].count().values\n",
    "    trans_df['trans_'+column+'_nunique'] = train_trans.groupby('user')[column].nunique().values\n",
    "#     des_df = train_trans.groupby('user')[column].describe()\n",
    "#     trans_df['trans_'+column+'_count'] = des_df['count'].values\n",
    "#     trans_df['trans_'+column+'_n'] = des_df['unique'].values\n",
    "#     trans_df['trans_'+column+'_fre'] = des_df['freq'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for operate in ['max','min','mean','median','std']:\n",
    "    trans_df['trans_amount_'+operate]=train_trans.groupby(['user'])['amount'].agg(operate).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_trans['time_diff_sec'] = train_trans['tm_diff'].apply(timedelta2sec)\n",
    "\n",
    "for operate in ['max','min','mean','median','std']:\n",
    "    trans_df['trans_time_'+operate]=train_trans.groupby('user')['time_diff_sec'].agg(operate).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准差、freq有空值，用0填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "trans_df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "trans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "trans_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# train_df = df_base\n",
    "train_df = pd.merge(df_base, op_df, on='user', how='left')\n",
    "train_df = pd.merge(train_df, trans_df, on='user', how='left')\n",
    "\n",
    "train=train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "length = len(train)\n",
    "train_length = int(0.9*length)\n",
    "\n",
    "x = train.drop(columns=['user','label'],axis=1)[0:train_length]\n",
    "y = train['label'].values[0:train_length]\n",
    "valid_x = train.drop(columns=['user','label'],axis=1)[train_length:]\n",
    "valid_y = train['label'].values[train_length:]\n",
    "\n",
    "x = x.fillna(0)\n",
    "valid_x = valid_x.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model = LogisticRegression()\n",
    "# model.fit(x,y)\n",
    "\n",
    "model = LGBMClassifier(n_estimators=1000, \n",
    "                       learning_rate=0.045,\n",
    "                       subsample=0.8,\n",
    "                       colsample_bytree=0.8,\n",
    "                       reg_alpha=100,\n",
    "                       reg_lambda=100)\n",
    "model.fit(x, y,\n",
    "          eval_set=(valid_x, valid_y),\n",
    "          early_stopping_rounds=5\n",
    "          )\n",
    "\n",
    "prediction = model.predict_proba(valid_x)[:,1]\n",
    "auc = roc_auc_score(valid_y,prediction)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "joblib.dump(model,'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'column': x.columns,'importance': np.abs(model.coef_[0])}).sort_values(by='importance')[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'column': x.columns,'importance': model.feature_importances_}).sort_values(by='importance')[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# select_feature=importance_df['column']\n",
    "\n",
    "# x = x[select_feature]\n",
    "# valid_x = valid_x[select_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "# model = CatBoostClassifier(\n",
    "#     iterations=500,\n",
    "#     random_seed=42,\n",
    "#     logging_level='Silent'\n",
    "# )\n",
    "\n",
    "# model.fit(\n",
    "#     x.values, y,\n",
    "#     eval_set=(valid_x.values, valid_y),\n",
    "# #     logging_level='Verbose',  # you can uncomment this for text output\n",
    "#     plot=True\n",
    "# )\n",
    "\n",
    "# prediction = model.predict_proba(valid_x)[:,1]\n",
    "# auc = roc_auc_score(valid_y,prediction)\n",
    "# auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline:\n",
    "0.6769352059269422\n",
    "\n",
    "添加trans amount的扩展变量：\n",
    "0.6782946345739088\n",
    "\n",
    "添加trans ip3的扩展变量：\n",
    "0.6791192236191647\n",
    "\n",
    "添加trans type直接数值化：\n",
    "0.6796788342378327\n",
    "\n",
    "添加所有的des特征，并修改了学习率：\n",
    "0.6823851869318595\n",
    "\n",
    "添加avg特征：\n",
    "0.6831416556804824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}